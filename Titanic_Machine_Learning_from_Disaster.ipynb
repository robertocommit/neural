{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Titanic: Machine Learning from Disaster.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUECicWCrDgBYIU20j/EFL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robimalco/colab/blob/main/Titanic_Machine_Learning_from_Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Gab4RH7kRj"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCh6gWUa8Ue_"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp3MikqX8W3M"
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZqQJv1X8ZYi"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx98snIy8diy"
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xRj1RU8h_m",
        "outputId": "cfb4916a-e58d-468d-940f-893132ed4c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!kaggle competitions download -c titanic"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)\n",
            "test.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "gender_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO57DG0g80_w",
        "outputId": "363c1b62-8b41-41eb-f0ca-eefa44799134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!mkdir train\n",
        "!unzip train.zip -d train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘train’: File exists\n",
            "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LJhFeCF_Ie7"
      },
      "source": [
        "# START"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm5ybphV_KqE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "121_vnK4_q1c"
      },
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e457G6xW3Wyi",
        "outputId": "f784961a-2b1a-487b-b14b-5ac14830cc8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Diplay null values of each column\n",
        "train_df.isnull().sum()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmnt3N-g34l7"
      },
      "source": [
        "train_df = train_df[~train_df['Age'].isnull()]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ve3mxGL5RiW"
      },
      "source": [
        "train_df = train_df[~train_df['Embarked'].isnull()]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwfu7steBeOc"
      },
      "source": [
        "train_df['TicketCluster'] = train_df['Ticket'].str[0]\n",
        "train_df['TicketCluster'] = np.where(train_df[\"TicketCluster\"].str.isdigit(), \"X\", train_df[\"TicketCluster\"])"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC0eae5xEJOs"
      },
      "source": [
        "# train_df['TicketCluster'].hist(bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eiBZHlADF7E"
      },
      "source": [
        "train_df['AgeCluster'] = pd.cut(train_df['Age'], bins=[0, 5, 10, 20, 30, 40, 50, 60, 70, 81], include_lowest=True, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
        "train_df['FareCluster'] = pd.cut(train_df['Fare'], bins=[0, 51, 101, 2000], include_lowest=True, labels=[0, 1, 2])\n",
        "categorical_columns = ['Pclass', 'Sex', 'SibSp', 'Parch', 'AgeCluster', 'FareCluster', 'Embarked', 'TicketCluster']"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pT9WHiSIvG_"
      },
      "source": [
        "for column in categorical_columns:\n",
        "    train_df[column] = LabelEncoder().fit_transform(train_df[column])\n",
        "for column in categorical_columns:\n",
        "  train_df[column] = train_df[column].astype('category')"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAauVM0xKQYZ"
      },
      "source": [
        "pclass = train_df['Pclass'].cat.codes.values\n",
        "sex = train_df['Sex'].cat.codes.values\n",
        "sibsp = train_df['SibSp'].cat.codes.values\n",
        "parch = train_df['Parch'].cat.codes.values\n",
        "ageCluster = train_df['AgeCluster'].cat.codes.values\n",
        "fareCluster = train_df['FareCluster'].cat.codes.values\n",
        "embarked = train_df['Embarked'].cat.codes.values\n",
        "ticketCluster = train_df['TicketCluster'].cat.codes.values\n",
        "categorical_data = np.stack([pclass, sex, sibsp, parch, ageCluster, fareCluster, embarked, ticketCluster], 1)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00YhiAHUOWf6"
      },
      "source": [
        "tensor_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "tensor_output = torch.tensor(train_df['Survived'].values).flatten()"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6k2j2iWLdEm"
      },
      "source": [
        "categorical_columns_size = [len(train_df[column].cat.categories) for column in categorical_columns]\n",
        "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_columns_size]\n",
        "total_records = len(train_df)\n",
        "test_records = int(total_records * 0.2)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_BQPREIO9x6"
      },
      "source": [
        "tensor_train_data = tensor_data[:total_records-test_records]\n",
        "tensor_test_data = tensor_data[total_records-test_records:total_records]\n",
        "tensor_train_output = tensor_output[:total_records-test_records]\n",
        "tensor_test_output = tensor_output[total_records-test_records:total_records]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGmxMRzyPnPS"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, embedding_sizes):\n",
        "    super().__init__()\n",
        "    self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
        "    n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
        "    self.lin1 = nn.Linear(n_emb, 200)\n",
        "    self.lin2 = nn.Linear(200, 70)\n",
        "    self.lin3 = nn.Linear(70, 2)\n",
        "    self.bn1 = nn.BatchNorm1d(n_emb)\n",
        "    self.bn2 = nn.BatchNorm1d(200)\n",
        "    self.bn3 = nn.BatchNorm1d(70)\n",
        "    self.emb_drop = nn.Dropout(0.6)\n",
        "    self.drops = nn.Dropout(0.3)\n",
        "  def forward(self, x_cat):\n",
        "    x = [e(x_cat[:,i]) for i, e in enumerate(self.embeddings)]\n",
        "    x = torch.cat(x, 1)\n",
        "    x = self.emb_drop(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.lin1(x)\n",
        "    x = self.drops(x)\n",
        "    x = self.bn2(x)\n",
        "    x = F.relu(self.lin2(x))\n",
        "    x = self.drops(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.lin3(x)\n",
        "    return x"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TzMuqctTf0v"
      },
      "source": [
        "model = Model(categorical_embedding_sizes)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVCB45E088ol",
        "outputId": "806d98a2-9b93-46b3-d21b-8001fab9ee4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "epochs = 300\n",
        "aggregated_losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    i += 1\n",
        "    y_pred = model(tensor_train_data)\n",
        "    single_loss = loss_function(y_pred, tensor_train_output)\n",
        "\n",
        "    aggregated_losses.append(single_loss)\n",
        "    if i%100 == 1:\n",
        "        print(\"epoch: \" + str(i) + \"\\tloss: \" + str(single_loss.item()))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    single_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"epoch: \" + str(i) + \"\\tloss: \" + str(single_loss.item()))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1\tloss: 0.8375767469406128\n",
            "epoch: 101\tloss: 0.6243467926979065\n",
            "epoch: 201\tloss: 0.5678133368492126\n",
            "epoch: 300\tloss: 0.547076404094696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-NJLACm9yFI",
        "outputId": "b419e24c-d1d5-4556-bc5f-594861467e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    y_val = model(tensor_test_data)\n",
        "    loss = loss_function(y_val.squeeze(), tensor_test_output)\n",
        "print(\"Loss: \" + str(loss))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: tensor(0.5211)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCcHJKfr-511",
        "outputId": "30adc7ba-d7dc-44b7-b2c5-7c67fad4d7a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "y_val_sklearn = np.argmax(y_val, axis=1)\n",
        "\n",
        "print(\"\\nconfusion_matrix\")\n",
        "print(confusion_matrix(tensor_test_output, y_val_sklearn))\n",
        "print(\"\\nclassification_report\")\n",
        "print(classification_report(tensor_test_output, y_val_sklearn))\n",
        "print(\"\\naccuracy_score\")\n",
        "print(accuracy_score(tensor_test_output, y_val_sklearn))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "confusion_matrix\n",
            "[[75 12]\n",
            " [21 34]]\n",
            "\n",
            "classification_report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.86      0.82        87\n",
            "           1       0.74      0.62      0.67        55\n",
            "\n",
            "    accuracy                           0.77       142\n",
            "   macro avg       0.76      0.74      0.75       142\n",
            "weighted avg       0.76      0.77      0.76       142\n",
            "\n",
            "\n",
            "accuracy_score\n",
            "0.7676056338028169\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}