{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Home Credit Default Risk",
      "provenance": [],
      "collapsed_sections": [
        "s933KG08D48G"
      ],
      "authorship_tag": "ABX9TyMwvSFZ9rzZVv3Cmj962txA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robimalco/colab/blob/main/Home_Credit_Default_Risk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9Cm7RzTvoY"
      },
      "source": [
        "# ADD KEYS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPDViyQqPsaM"
      },
      "source": [
        "!rm -rf *\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"robimalco\"\n",
        "os.environ['KAGGLE_KEY'] = \"538b89051dfe95093bd46e9ca94d2202\"\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c home-credit-default-risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdr6gDQq5rET"
      },
      "source": [
        "# START SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KnrADYj7yUe"
      },
      "source": [
        "!unzip application_test.csv.zip\n",
        "!unzip application_train.csv.zip\n",
        "!unzip previous_application.csv.zip\n",
        "# !unzip POS_CASH_balance.csv.zip\n",
        "# !unzip bureau.csv.zip\n",
        "# !unzip bureau_balance.csv.zip\n",
        "# !unzip credit_card_balance.csv.zip\n",
        "# !unzip installments_payments.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbHvWyux-QjF"
      },
      "source": [
        "!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import string\n",
        "letters = string.ascii_lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWBMpGHRRgn-"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGDv_sUBXxI"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnY9FfmTaHux"
      },
      "source": [
        "!pip install --upgrade gspread\n",
        "from google.colab import auth, drive\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "drive.mount('/drive')\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1M-CqsTbBu7ScY4mZBcpI8kPbI7F-BE8aPTC4UknumYk/edit#gid=0')\n",
        "sheet = wb.worksheet('Data')\n",
        "def get_next_row(worksheet):\n",
        "    str_list = list(filter(None, worksheet.col_values(1)))\n",
        "    return str(len(str_list)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s933KG08D48G"
      },
      "source": [
        "# UTILITIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1r_TCDSbOWB"
      },
      "source": [
        "def generate_timestamp():\n",
        "  now = datetime.datetime.now()\n",
        "  year = '{:02d}'.format(now.year)\n",
        "  month = '{:02d}'.format(now.month)\n",
        "  day = '{:02d}'.format(now.day)\n",
        "  hour = '{:02d}'.format(now.hour)\n",
        "  minute = '{:02d}'.format(now.minute)\n",
        "  return '{}-{}-{} {}:{}'.format(year, month, day, hour, minute)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlhVH--sUJ9u"
      },
      "source": [
        "def order_columns_alphabetically(input_df):\n",
        "  input_df_columns = list(input_df.columns)\n",
        "  input_df_columns.sort()\n",
        "  return input_df[input_df_columns]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbnyF3dTGWo"
      },
      "source": [
        "def smart_overview(input_df):\n",
        "  a_types = []\n",
        "  a_countUnique = []\n",
        "  a_missing = []\n",
        "  a_missing_perc = []\n",
        "  a_corrTarget = []\n",
        "  a_min = []\n",
        "  a_max = []\n",
        "  a_mean = []\n",
        "  a_median = []\n",
        "  a_quantile = []\n",
        "  for column in input_df.columns:\n",
        "    x = input_df[column]\n",
        "    x_type = input_df.dtypes[column]\n",
        "    countUnique = len(x.unique())\n",
        "    missing = x.isnull().sum()\n",
        "    missing_perc = round((missing/input_df.shape[0]),3)*100\n",
        "    if x_type == np.int64 or x_type == np.float64:\n",
        "      if 'TARGET' in input_df.columns:\n",
        "        a_corrTarget.append(round(x.corr(input_df['TARGET']), 3))\n",
        "      else:\n",
        "        a_corrTarget.append('/')\n",
        "      a_min.append(x.min())\n",
        "      a_max.append(x.max())\n",
        "      a_mean.append(x.mean())\n",
        "      a_median.append(x.median())\n",
        "      a_quantile.append(x.quantile(0.5))\n",
        "    else:\n",
        "      a_corrTarget.append('')\n",
        "      a_min.append('')\n",
        "      a_max.append('')\n",
        "      a_mean.append('')\n",
        "      a_median.append('')\n",
        "      a_quantile.append('')\n",
        "    a_types.append(x_type)\n",
        "    a_countUnique.append(countUnique)\n",
        "    a_missing.append(missing)\n",
        "    a_missing_perc.append(missing_perc)\n",
        "  explore_df = pd.DataFrame({\n",
        "    'Columns': input_df.columns,\n",
        "    'Types': a_types,\n",
        "    'Unique': a_countUnique,\n",
        "    'Missing': a_missing,\n",
        "    'Missing%': a_missing_perc,\n",
        "    'CorrTarget': a_corrTarget,\n",
        "    'Min': a_min,\n",
        "    'Max': a_max,\n",
        "    'Mean': a_mean,\n",
        "    'Median': a_median,\n",
        "    'Quantile': a_quantile\n",
        "  })\n",
        "  explore_df.set_index('Columns', inplace=True)\n",
        "  return explore_df.transpose()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxeWyJnq58bz"
      },
      "source": [
        "# Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0oEbaFhnLeC"
      },
      "source": [
        "# application_{train|test}.csv --> main table, static data for all applications. One row represents one loan in our data sample.\n",
        "# bureau.csv --> client's previous credits, for every loan in our sample, there are as many rows as number of credits the client had.\n",
        "# bureau_balance.csv --> monthly balances of previous credits, one row for each month.\n",
        "# POS_CASH_balance.csv --> monthly balance snapshots of previous point of sales and cash loans that the applicant had, one row for each month.\n",
        "# credit_card_balance.csv --> monthly balance snapshots of previous credit cards, one row for each month.\n",
        "# previous_application.csv --> all previous applications for Home Credit loans of clients who have loans.\n",
        "# installments_payments.csv --> repayment history for the previously disbursed credits.\n",
        "\n",
        "columns_descriptions_df = pd.read_csv('HomeCredit_columns_description.csv', engine='python')\n",
        "columns_descriptions_df[columns_descriptions_df['Table'] == 'previous_application.csv'].sort_values(by=['Row'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ4CWCK1cvvb"
      },
      "source": [
        "# SET HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n56O92gqm0Xd"
      },
      "source": [
        "hp_test_size = 0.2\n",
        "hp_emb_drop = 0.04\n",
        "hp_layers = [800, 400]\n",
        "hp_ps = [0.001,0.01]\n",
        "hp_lr= 0.001\n",
        "hp_epochs = 3"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iS2krKz9hVR"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV9Yv1D_m6Mv"
      },
      "source": [
        "application_train_df = pd.read_csv('application_train.csv').sample(frac = 1)\n",
        "application_test_df = pd.read_csv('application_test.csv')\n",
        "previous_application_df = pd.read_csv('previous_application.csv')\n",
        "# bureau_df = pd.read_csv('bureau.csv')\n",
        "# bureau_balance_df = pd.read_csv('bureau_balance.csv')\n",
        "# pos_cash_balance_df = pd.read_csv('POS_CASH_balance.csv')\n",
        "# credit_card_balance_df = pd.read_csv('credit_card_balance.csv')\n",
        "# installments_payments_df = pd.read_csv('installments_payments.csv')"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzo49Q4onAU8"
      },
      "source": [
        "application_train_df['CSV_SOURCE'] = 'application_train.csv'\n",
        "application_test_df['CSV_SOURCE'] = 'application_test.csv'\n",
        "df = pd.concat([application_train_df, application_test_df])"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPHertNiEHbj"
      },
      "source": [
        "# MANAGE previous_application.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSa8Qpd9nFf2"
      },
      "source": [
        "temp_previous_df = previous_application_df.groupby('SK_ID_CURR', as_index=False).agg({'NAME_CONTRACT_STATUS': lambda x: ','.join(set(','.join(x).split(',')))})\n",
        "temp_previous_df['has_only_approved'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'] == 'Approved', '1', '0')\n",
        "temp_previous_df['has_been_rejected'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'].str.contains('Refused'), '1', '0')"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAeIVIZP8vs2"
      },
      "source": [
        "# JOIN DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcYO75sMmvoc"
      },
      "source": [
        "df = pd.merge(df, temp_previous_df, on='SK_ID_CURR', how='left')"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH_rbDvupjn4"
      },
      "source": [
        "# CREATE CUSTOM COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCJDMOHhoud3"
      },
      "source": [
        "df['TOTAL_AMT_REQ_CREDIT_BUREAU_YEAR'] = (\n",
        "  df['AMT_REQ_CREDIT_BUREAU_YEAR'] * 0.1 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_QRT'] * 0.2 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_MON'] * 0.8 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_WEEK'] * 2 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_DAY'] * 4 +\n",
        "  df['AMT_REQ_CREDIT_BUREAU_HOUR'] * 8)"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmxzAril9NMB"
      },
      "source": [
        "# MANAGE COLUMNS (NUMERICAL VS CATEGORICAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEy-9Qsz7ZoM"
      },
      "source": [
        "\"\"\"\n",
        "numerical_columns = [\n",
        "  'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n",
        "  'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION',\n",
        "  'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'DAYS_EMPLOYED', 'DAYS_LAST_PHONE_CHANGE',\n",
        "  'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "  'TOTAL_AMT_REQ_CREDIT_BUREAU_YEAR']\n",
        "categorical_columns = [\n",
        "  'CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "  'FLAG_EMAIL', 'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'LIVE_CITY_NOT_WORK_CITY',\n",
        "  'LIVE_REGION_NOT_WORK_REGION', 'NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE',\n",
        "  'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE',\n",
        "  'REGION_RATING_CLIENT_W_CITY', 'has_only_approved', 'has_been_rejected']\n",
        "\"\"\"\n",
        "numerical_columns = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL', 'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH']\n",
        "categorical_columns = ['CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'has_only_approved', 'has_been_rejected']\n",
        "target_column = ['TARGET']\n",
        "df = df[numerical_columns + categorical_columns + target_column]"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDybyT2XAc5P"
      },
      "source": [
        "smart_overview(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96wo_5oYYA3"
      },
      "source": [
        "# MANAGE MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2j9zI7mHBoP"
      },
      "source": [
        "for numerical_column in numerical_columns:\n",
        "  df[numerical_column].fillna(value=df[numerical_column].median(), inplace=True)\n",
        "\n",
        "for categorical_column in categorical_columns:\n",
        "  df[categorical_column].fillna('NULL', inplace=True)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMm7CCwYgi0"
      },
      "source": [
        "# STANDARDISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT7t1r068wJX"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df[numerical_columns] = pd.DataFrame(min_max_scaler.fit_transform(df[numerical_columns]))"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66MJfs5ZBUC"
      },
      "source": [
        "# CONVERT CATEGORICAL COLUMNS INTO TYPE \"CATEGORY\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Hv3Ii6DWMx"
      },
      "source": [
        "categorical_columns.remove('CSV_SOURCE')\n",
        "\n",
        "for column in categorical_columns:\n",
        "  df[column] = LabelEncoder().fit_transform(df[column].astype(str))\n",
        "  df[column] = df[column].astype('category')"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyAJPY3hGJI-"
      },
      "source": [
        "smart_overview(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lmf7AyiaqGL"
      },
      "source": [
        "# SPLIT DATA INTO TRAINING vs TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6lrtUpJan-"
      },
      "source": [
        "train_df = df[df['CSV_SOURCE'] == 'application_train.csv']\n",
        "train_output_df = pd.DataFrame(train_df['TARGET'], columns=['TARGET'])\n",
        "\n",
        "test_df = df[df['CSV_SOURCE'] == 'application_test.csv']"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7yzRWz9a-UZ"
      },
      "source": [
        "# REMOVE NOT USEFUL COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKJ3_8sMbC_U"
      },
      "source": [
        "train_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)\n",
        "test_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XyQsFRcpSZ"
      },
      "source": [
        "# CREATE VALIDATION SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isjke6nLcV2G"
      },
      "source": [
        "x_train, x_validation, y_train, y_validation = train_test_split(train_df, train_output_df, test_size=hp_test_size, random_state=42)"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5OXy6FbTUR"
      },
      "source": [
        "# CREATE TENSORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8ZS9Lidh3R"
      },
      "source": [
        "def create_tensors(input_df):\n",
        "  stack = []\n",
        "  for column in input_df.columns:\n",
        "    if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
        "      stack.append(input_df[column].astype(np.float64))\n",
        "    else:\n",
        "      stack.append(input_df[column].cat.codes.values)\n",
        "  return torch.tensor(np.stack(stack, 1), dtype=torch.float)\n",
        "\n",
        "tensor_x_train_cat = create_tensors(x_train[categorical_columns]).float().to(device)\n",
        "tensor_x_train_num = create_tensors(x_train[numerical_columns]).float().to(device)\n",
        "tensor_y_train = torch.tensor(y_train.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_valid_cat = create_tensors(x_validation[categorical_columns]).float().to(device)\n",
        "tensor_x_valid_num = create_tensors(x_validation[numerical_columns]).float().to(device)\n",
        "tensor_y_valid = torch.tensor(y_validation.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_test_cat = create_tensors(test_df[categorical_columns]).float().to(device)\n",
        "tensor_x_test_num = create_tensors(test_df[numerical_columns]).float().to(device)"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRUVJlngYeT"
      },
      "source": [
        "# CREATE CATEGORICAL EMBEDDING SIZES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSDav1gcL2MN"
      },
      "source": [
        "categorical_columns_size = [len(df[column].cat.categories) for column in categorical_columns]\n",
        "categorical_embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_columns_size]"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJEKKoLhQAz"
      },
      "source": [
        "# CREATE PYTORCH DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaVwMCL64ttq"
      },
      "source": [
        "train_tensor_dataset = TensorDataset(tensor_x_train_cat, tensor_x_train_num, tensor_y_train)\n",
        "train_loader = DataLoader(dataset=train_tensor_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsI8RQbZhriV"
      },
      "source": [
        "# DEFINE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHX1GpfYkdHd"
      },
      "source": [
        "![](https://yashuseth.files.wordpress.com/2018/07/model1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1m9m_3Msst"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, embedding_size, input_size, num_numerical_cols, layers, ps):\n",
        "    super().__init__()\n",
        "\n",
        "    self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
        "    self.emb_drop = nn.Dropout(hp_emb_drop)\n",
        "\n",
        "    self.bn_cont = nn.BatchNorm1d(num_numerical_cols)\n",
        "\n",
        "    layerlist = []\n",
        "    for i, elem in enumerate(layers):\n",
        "      layerlist.append(nn.Linear(input_size, elem))\n",
        "      layerlist.append(nn.ReLU(inplace=True))\n",
        "      layerlist.append(nn.BatchNorm1d(layers[i]))\n",
        "      layerlist.append(nn.Dropout(ps[i]))\n",
        "      input_size = elem\n",
        "    layerlist.append(nn.Linear(layers[-1], 1))\n",
        "\n",
        "    self.layers = nn.Sequential(*layerlist)\n",
        "\n",
        "  def forward(self, x_c, x_n):\n",
        "\n",
        "    embeddings = [e(x_c[:,i].long()) for i, e in enumerate(self.all_embeddings)]\n",
        "\n",
        "    x = torch.cat(embeddings, 1)\n",
        "    x = self.emb_drop(x)\n",
        "\n",
        "    x_n = self.bn_cont(x_n)\n",
        "\n",
        "    x = torch.cat([x, x_n], 1)\n",
        "    x = self.layers(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHlbmRfnhjA"
      },
      "source": [
        "# INSTANTIATE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBAifb1TTjhG"
      },
      "source": [
        "num_numerical_cols = tensor_x_train_num.shape[1]\n",
        "\n",
        "num_categorical_cols = sum((nf for ni, nf in categorical_embedding_sizes))\n",
        "initial_input_size = num_categorical_cols + num_numerical_cols\n",
        "\n",
        "model = Model(categorical_embedding_sizes, initial_input_size, num_numerical_cols, layers=hp_layers, ps=hp_ps)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hp_lr)\n",
        "model.to(device)\n",
        "tot_losses = []"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTtyHh92n4Ac"
      },
      "source": [
        "# TRAIN NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yyOYgr5bZP"
      },
      "source": [
        "start_training = generate_timestamp()\n",
        "\n",
        "for epoch in range(hp_epochs):\n",
        "  train_losses = []\n",
        "  for x_cat, x_num, y in train_loader:\n",
        "    y_pred = model(x_cat, x_num)\n",
        "    single_loss = torch.sqrt(loss_function(y_pred.squeeze(), y))\n",
        "    optimizer.zero_grad()\n",
        "    single_loss.backward() \n",
        "    optimizer.step()\n",
        "    train_losses.append(single_loss.item())\n",
        "  epoch_loss = 1.0 * sum(train_losses) / len(train_losses)\n",
        "  tot_losses.append(epoch_loss)\n",
        "  print(\"epoch: \" + str(epoch) + \"\\tloss: \" + str(epoch_loss))\n",
        "plt.plot(tot_losses)\n",
        "plt.show()\n",
        "\n",
        "last_train_loss = epoch_loss\n",
        "end_training = generate_timestamp()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88QHnDQvGnQ"
      },
      "source": [
        "# VALIDATE NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozr6OavjNgrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443c7e17-412d-48ab-9869-0dbba1abb645"
      },
      "source": [
        "validation_tensor_dataset = TensorDataset(tensor_x_valid_cat, tensor_x_valid_num, tensor_y_valid)\n",
        "validation_loader = DataLoader(dataset=validation_tensor_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "valid_losses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for x_cat, x_num, y in validation_loader:\n",
        "    y_valid = model(x_cat, x_num)\n",
        "    validation_loss = torch.sqrt(loss_function(y_valid.squeeze(), y))\n",
        "    valid_losses.append(validation_loss.item())\n",
        "  valid_loss = round(1.0 * sum(valid_losses) / len(valid_losses), 5)\n",
        "  print(\"loss: \" + str(valid_loss))"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.23923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9QC2cAvNrS"
      },
      "source": [
        "# MAKE PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbY5ybTLJpd4"
      },
      "source": [
        "with torch.no_grad():\n",
        "  y_test = model(tensor_x_test_cat, tensor_x_test_num)\n",
        "pd.DataFrame(y_test).astype(\"float\").hist(bins=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5CHX5POvVYB"
      },
      "source": [
        "# SAVE PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc2BwhwKoHDa"
      },
      "source": [
        "prediction_id = ''.join(random.choice(letters) for i in range(5))\n",
        "prediction_df = pd.DataFrame(y_test).astype(\"float\")\n",
        "x_scaled = min_max_scaler.fit_transform(prediction_df)\n",
        "prediction_df = pd.DataFrame(x_scaled)\n",
        "prediction_df = pd.concat([prediction_df, application_test_df['SK_ID_CURR']], axis=1)\n",
        "prediction_df.columns = ['TEMP_TARGET', 'SK_ID_CURR']\n",
        "prediction_df['TARGET'] = round(prediction_df['TEMP_TARGET'], 1)\n",
        "prediction_df = prediction_df[['SK_ID_CURR', 'TARGET']]\n",
        "prediction_df.to_csv('/drive/My Drive/pycharm_colab_training/kaggle/HomeCreditDefaultRisk/submissions/' + prediction_id + '.csv', index=False)\n",
        "test_target_mean = str(round(prediction_df['TARGET'].mean(), 3))\n",
        "test_distrbution = prediction_df.groupby(by=['TARGET'])['TARGET'].count()\n",
        "print(\"test_target_mean:\", test_target_mean)\n",
        "print(test_distrbution.to_string(header=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg2sCUB3aOdy"
      },
      "source": [
        "# SAVE DATA TO SHEET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA80EJskMH6E"
      },
      "source": [
        "model_values_dict = {\n",
        "  'ID': prediction_id,\n",
        "  'start_training': start_training,\n",
        "  'end_training': end_training,\n",
        "  'perc_test_size': hp_test_size,\n",
        "  'emb_drop': hp_emb_drop,\n",
        "  'layers': '\\n'.join([str(i) for i in hp_layers]),\n",
        "  'ps': '\\n'.join([str(i) for i in hp_ps]),\n",
        "  'lr': hp_lr,\n",
        "  'epochs': hp_epochs,\n",
        "  'train_losses': '\\n'.join([str(round(i, 5)) for i in tot_losses]),\n",
        "  'last_train_loss': last_train_loss,\n",
        "  'valid_loss': valid_loss,\n",
        "  'Δloss%': str(round((valid_loss / epoch_loss - 1) * 100, 3)) + '%',\n",
        "  'test_target_mean': test_target_mean,\n",
        "  'test_distrbution': test_distrbution.to_string(header=False),\n",
        "  'numerical_columns': len(numerical_columns),\n",
        "  'categorical_columns': len(categorical_columns),\n",
        "  'model_parameters': str(model.parameters)\n",
        "}\n",
        "\n",
        "next_row = get_next_row(sheet)\n",
        "cells = sheet.range('A' + next_row + ':R' + next_row)\n",
        "model_values_list = list(model_values_dict.values())\n",
        "for i, cell in enumerate(cells):\n",
        "  cell.value = model_values_list[i]\n",
        "sheet.update_cells(cells)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}