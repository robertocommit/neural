{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Home Credit Default Risk",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbkvJKtGuLYAV2HUBCpI2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robimalco/colab/blob/main/Home_Credit_Default_Risk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9Cm7RzTvoY"
      },
      "source": [
        "# ADD KEYS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPDViyQqPsaM"
      },
      "source": [
        "!rm -rf *\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"robimalco\"\n",
        "os.environ['KAGGLE_KEY'] = \"\"\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c home-credit-default-risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdr6gDQq5rET"
      },
      "source": [
        "# START SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KnrADYj7yUe"
      },
      "source": [
        "!unzip application_test.csv.zip\n",
        "!unzip application_train.csv.zip\n",
        "!unzip previous_application.csv.zip\n",
        "# !unzip POS_CASH_balance.csv.zip\n",
        "# !unzip bureau.csv.zip\n",
        "# !unzip bureau_balance.csv.zip\n",
        "# !unzip credit_card_balance.csv.zip\n",
        "# !unzip installments_payments.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbHvWyux-QjF"
      },
      "source": [
        "!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "import datetime\n",
        "import random\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWBMpGHRRgn-"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGDv_sUBXxI"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnY9FfmTaHux"
      },
      "source": [
        "!pip install --upgrade gspread\n",
        "from google.colab import auth, drive, files\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "drive.mount('/drive')\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1M-CqsTbBu7ScY4mZBcpI8kPbI7F-BE8aPTC4UknumYk/edit#gid=0')\n",
        "sheet = wb.worksheet('Data')\n",
        "def get_next_row(worksheet):\n",
        "    str_list = list(filter(None, worksheet.col_values(1)))\n",
        "    return str(len(str_list)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s933KG08D48G"
      },
      "source": [
        "# UTILITIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1r_TCDSbOWB"
      },
      "source": [
        "def generate_timestamp():\n",
        "  now = datetime.datetime.now()\n",
        "  year = '{:02d}'.format(now.year)\n",
        "  month = '{:02d}'.format(now.month)\n",
        "  day = '{:02d}'.format(now.day)\n",
        "  hour = '{:02d}'.format(now.hour)\n",
        "  minute = '{:02d}'.format(now.minute)\n",
        "  return '{}-{}-{} {}:{}'.format(year, month, day, hour, minute)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlhVH--sUJ9u"
      },
      "source": [
        "def order_columns_alphabetically(input_df):\n",
        "  input_df_columns = list(input_df.columns)\n",
        "  input_df_columns = sorted(input_df_columns, key=str.casefold)\n",
        "  return input_df[input_df_columns]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbnyF3dTGWo"
      },
      "source": [
        "def smart_overview(input_df):\n",
        "  a_types = []\n",
        "  a_countUnique = []\n",
        "  a_missing = []\n",
        "  a_missing_perc = []\n",
        "  a_corrTarget = []\n",
        "  a_min = []\n",
        "  a_max = []\n",
        "  a_mean = []\n",
        "  a_median = []\n",
        "  a_quantile = []\n",
        "  for column in input_df.columns:\n",
        "    x = input_df[column]\n",
        "    x_type = input_df.dtypes[column]\n",
        "    countUnique = len(x.unique())\n",
        "    missing = x.isnull().sum()\n",
        "    missing_perc = round((missing/input_df.shape[0]),3)*100\n",
        "    if x_type == np.int64 or x_type == np.float64:\n",
        "      if 'TARGET' in input_df.columns:\n",
        "        a_corrTarget.append(round(x.corr(input_df['TARGET']), 3))\n",
        "      else:\n",
        "        a_corrTarget.append('/')\n",
        "      a_min.append(x.min())\n",
        "      a_max.append(x.max())\n",
        "      a_mean.append(x.mean())\n",
        "      a_median.append(x.median())\n",
        "      a_quantile.append(x.quantile(0.5))\n",
        "    else:\n",
        "      a_corrTarget.append('')\n",
        "      a_min.append('')\n",
        "      a_max.append('')\n",
        "      a_mean.append('')\n",
        "      a_median.append('')\n",
        "      a_quantile.append('')\n",
        "    a_types.append(x_type)\n",
        "    a_countUnique.append(countUnique)\n",
        "    a_missing.append(missing)\n",
        "    a_missing_perc.append(missing_perc)\n",
        "  explore_df = pd.DataFrame({\n",
        "    'Columns': input_df.columns,\n",
        "    'Types': a_types,\n",
        "    'Unique': a_countUnique,\n",
        "    'Missing': a_missing,\n",
        "    'Missing%': a_missing_perc,\n",
        "    'CorrTarget': a_corrTarget,\n",
        "    'Min': a_min,\n",
        "    'Max': a_max,\n",
        "    'Mean': a_mean,\n",
        "    'Median': a_median,\n",
        "    'Quantile': a_quantile\n",
        "  })\n",
        "  explore_df.set_index('Columns', inplace=True)\n",
        "  return order_columns_alphabetically(explore_df.transpose())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxeWyJnq58bz"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0oEbaFhnLeC"
      },
      "source": [
        "# application_{train|test}.csv --> main table, static data for all applications. One row represents one loan in our data sample.\n",
        "# bureau.csv --> client's previous credits, for every loan in our sample, there are as many rows as number of credits the client had.\n",
        "# bureau_balance.csv --> monthly balances of previous credits, one row for each month.\n",
        "# POS_CASH_balance.csv --> monthly balance snapshots of previous point of sales and cash loans that the applicant had, one row for each month.\n",
        "# credit_card_balance.csv --> monthly balance snapshots of previous credit cards, one row for each month.\n",
        "# previous_application.csv --> all previous applications for Home Credit loans of clients who have loans.\n",
        "# installments_payments.csv --> repayment history for the previously disbursed credits.\n",
        "\n",
        "columns_descriptions_df = pd.read_csv('HomeCredit_columns_description.csv', engine='python')\n",
        "# columns_descriptions_df[columns_descriptions_df['Table'] == 'previous_application.csv'].sort_values(by=['Row'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw6-Dr8JWRtL"
      },
      "source": [
        "- The number of hidden neurons should be between the size of the input layer and the size of the output layer.\t\t\t\t\t\n",
        "- The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\t\t\t\t\t\n",
        "- The number of hidden neurons should be less than twice the size of the input layer.\n",
        "- I = 2000\n",
        "- H = 1321\n",
        "- O = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ4CWCK1cvvb"
      },
      "source": [
        "# SET HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n56O92gqm0Xd"
      },
      "source": [
        "hp_test_size = 0.2\n",
        "hp_epochs = 20\n",
        "hr_batch_size = 320\n",
        "hp_lr= 0.000005\n",
        "hp_emb_drop = 0.04\n",
        "hp_layers = [800, 350]\n",
        "hp_ps = [0.001,0.01]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iS2krKz9hVR"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV9Yv1D_m6Mv"
      },
      "source": [
        "application_train_df = pd.read_csv('application_train.csv').sample(frac = 1)\n",
        "application_test_df = pd.read_csv('application_test.csv')\n",
        "previous_application_df = pd.read_csv('previous_application.csv')\n",
        "# bureau_df = pd.read_csv('bureau.csv')\n",
        "# bureau_balance_df = pd.read_csv('bureau_balance.csv')\n",
        "# pos_cash_balance_df = pd.read_csv('POS_CASH_balance.csv')\n",
        "# credit_card_balance_df = pd.read_csv('credit_card_balance.csv')\n",
        "# installments_payments_df = pd.read_csv('installments_payments.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzo49Q4onAU8"
      },
      "source": [
        "application_train_df['CSV_SOURCE'] = 'application_train.csv'\n",
        "application_test_df['CSV_SOURCE'] = 'application_test.csv'\n",
        "df = pd.concat([application_train_df, application_test_df])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZVBDMZt5Lu-"
      },
      "source": [
        "correlations = df[['EXT_SOURCE_1','EXT_SOURCE_2', 'EXT_SOURCE_3', 'TARGET']].corr()['TARGET'].sort_values()\n",
        "correlations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxU41L1HitA4"
      },
      "source": [
        "# MANAGE previous_application.csv¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVMV-5AkxTlz"
      },
      "source": [
        "temp_previous_df = previous_application_df.groupby('SK_ID_CURR', as_index=False).agg({'NAME_CONTRACT_STATUS': lambda x: ','.join(set(','.join(x).split(',')))})\n",
        "temp_previous_df['has_only_approved'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'] == 'Approved', '1', '0')\n",
        "temp_previous_df['has_been_rejected'] = np.where(temp_previous_df['NAME_CONTRACT_STATUS'].str.contains('Refused'), '1', '0')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ljOVM3ixvu"
      },
      "source": [
        "# JOIN DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgk9nohHi3oH"
      },
      "source": [
        "df = pd.merge(df, temp_previous_df, on='SK_ID_CURR', how='left')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u99zYJrOi6DF"
      },
      "source": [
        "# CREATE CUSTOM COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VnZSw82i7ch"
      },
      "source": [
        "#################################################### total_amt_req_credit_bureau\n",
        "df['total_amt_req_credit_bureau'] = (\n",
        "  df['AMT_REQ_CREDIT_BUREAU_YEAR'] * 1 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_QRT'] * 2 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_MON'] * 8 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_WEEK'] * 16 + \n",
        "  df['AMT_REQ_CREDIT_BUREAU_DAY'] * 32 +\n",
        "  df['AMT_REQ_CREDIT_BUREAU_HOUR'] * 64)\n",
        "\n",
        "#######################################################################  has_job\n",
        "df['has_job'] = np.where(df['NAME_INCOME_TYPE'].isin(['Pensioner', 'Student', 'Unemployed']), '1', '0')\n",
        "\n",
        "#######################################################################  has_children\n",
        "df['has_children'] = np.where(df['CNT_CHILDREN'] > 0, '1', '0')\n",
        "\n",
        "####################################################### clusterise_days_employed\n",
        "def clusterise_days_employed(x):\n",
        "    days = x['DAYS_EMPLOYED']\n",
        "    if days > 0:\n",
        "      return 'not available'\n",
        "    else:\n",
        "      days = abs(days)\n",
        "      if days < 30:\n",
        "        return 'less 1 month'\n",
        "      elif days < 180:\n",
        "        return 'less 6 months'\n",
        "      elif days < 365:\n",
        "        return 'less 1 year'\n",
        "      elif days < 1095:\n",
        "        return 'less 3 years'\n",
        "      elif days < 1825:\n",
        "        return 'less 5 years'\n",
        "      elif days < 3600:\n",
        "        return 'less 10 years'\n",
        "      elif days < 7200:\n",
        "        return 'less 20 years'\n",
        "      elif days >= 7200:\n",
        "        return 'more 20 years'\n",
        "      else:\n",
        "        return 'not available'\n",
        "df['cluster_days_employed'] = df.apply(clusterise_days_employed, axis=1)\n",
        "\n",
        "#######################################################################  custom_ext_source_3\n",
        "def clusterise_ext_source(x):\n",
        "    if str(x) == 'nan':\n",
        "      return 'not available'\n",
        "    else:\n",
        "      if x < 0.1:\n",
        "        return 'less 0.1'\n",
        "      elif x < 0.2:\n",
        "        return 'less 0.2'\n",
        "      elif x < 0.3:\n",
        "        return 'less 0.3'\n",
        "      elif x < 0.4:\n",
        "        return 'less 0.4'\n",
        "      elif x < 0.5:\n",
        "        return 'less 0.5'\n",
        "      elif x < 0.6:\n",
        "        return 'less 0.6'\n",
        "      elif x < 0.7:\n",
        "        return 'less 0.7'\n",
        "      elif x < 0.8:\n",
        "        return 'less 0.8'\n",
        "      elif x < 0.9:\n",
        "        return 'less 0.9'\n",
        "      elif x <= 1:\n",
        "        return 'less 1'\n",
        "df['clusterise_ext_source_1'] = df['EXT_SOURCE_1'].apply(lambda x: clusterise_ext_source(x))\n",
        "df['clusterise_ext_source_2'] = df['EXT_SOURCE_2'].apply(lambda x: clusterise_ext_source(x))\n",
        "df['clusterise_ext_source_3'] = df['EXT_SOURCE_3'].apply(lambda x: clusterise_ext_source(x))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr8wgNfmksOF"
      },
      "source": [
        "# MANAGE COLUMNS (NUMERICAL VS CATEGORICAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_JkdEyjHj4"
      },
      "source": [
        "\"\"\"\n",
        "numerical_columns = [\n",
        "  'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'AMT_INCOME_TOTAL',\n",
        "  'REGION_POPULATION_RELATIVE', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION',\n",
        "  'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'DAYS_EMPLOYED', 'DAYS_LAST_PHONE_CHANGE',\n",
        "  'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3',\n",
        "  'TOTAL_AMT_REQ_CREDIT_BUREAU_YEAR']\n",
        "categorical_columns = [\n",
        "  'CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',\n",
        "  'FLAG_EMAIL', 'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'LIVE_CITY_NOT_WORK_CITY',\n",
        "  'LIVE_REGION_NOT_WORK_REGION', 'NAME_EDUCATION_TYPE', 'NAME_HOUSING_TYPE',\n",
        "  'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE',\n",
        "  'REGION_RATING_CLIENT_W_CITY', 'has_only_approved', 'has_been_rejected']\n",
        "\n",
        "  #########\n",
        "\n",
        "numerical_columns = ['AMT_ANNUITY', 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH'] \n",
        "categorical_columns = [\n",
        "  'CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'NAME_EDUCATION_TYPE',\n",
        "  'FLAG_OWN_REALTY', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE',\n",
        "  'has_only_approved', 'has_been_rejected', 'has_job', 'cluster_days_employed']\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "numerical_columns = [\n",
        "  'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_ID_PUBLISH']\n",
        "categorical_columns = [\n",
        "  'CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'NAME_EDUCATION_TYPE', 'FLAG_OWN_REALTY', 'OCCUPATION_TYPE', 'ORGANIZATION_TYPE',\n",
        "  'has_only_approved', 'has_been_rejected', 'has_job', 'has_children', 'cluster_days_employed',\n",
        "  'clusterise_ext_source_1', 'clusterise_ext_source_2', 'clusterise_ext_source_3']\n",
        "\n",
        "\n",
        "target_column = ['TARGET']\n",
        "df = df[numerical_columns + categorical_columns + target_column]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDINhubzjH8h"
      },
      "source": [
        "smart_overview(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FYGZN2vjNWm"
      },
      "source": [
        "# MANAGE MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LrLlgkujMls"
      },
      "source": [
        "for numerical_column in numerical_columns:\n",
        "  if df[numerical_column].isnull().values.any():\n",
        "    df[numerical_column + '_isnull'] = np.where(df[numerical_column].isnull(), '1', '0')\n",
        "  df[numerical_column].fillna(value=df[numerical_column].median(), inplace=True)\n",
        "\n",
        "for categorical_column in categorical_columns:\n",
        "  df[categorical_column].fillna('NULL', inplace=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMm7CCwYgi0"
      },
      "source": [
        "# STANDARDISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT7t1r068wJX"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df[numerical_columns] = pd.DataFrame(min_max_scaler.fit_transform(df[numerical_columns]))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66MJfs5ZBUC"
      },
      "source": [
        "# CONVERT CATEGORICAL COLUMNS INTO TYPE \"CATEGORY\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Hv3Ii6DWMx"
      },
      "source": [
        "categorical_columns.remove('CSV_SOURCE')\n",
        "\n",
        "for column in categorical_columns:\n",
        "  df[column] = LabelEncoder().fit_transform(df[column].astype(str))\n",
        "  df[column] = df[column].astype('category')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWQbsqwt0mWK"
      },
      "source": [
        "smart_overview(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lmf7AyiaqGL"
      },
      "source": [
        "# SPLIT DATA INTO TRAINING vs TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6lrtUpJan-"
      },
      "source": [
        "train_df = df[df['CSV_SOURCE'] == 'application_train.csv']\n",
        "train_output_df = pd.DataFrame(train_df['TARGET'], columns=['TARGET'])\n",
        "\n",
        "test_df = df[df['CSV_SOURCE'] == 'application_test.csv']"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7yzRWz9a-UZ"
      },
      "source": [
        "# REMOVE NOT USEFUL COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKJ3_8sMbC_U"
      },
      "source": [
        "train_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)\n",
        "test_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XyQsFRcpSZ"
      },
      "source": [
        "# CREATE VALIDATION SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isjke6nLcV2G"
      },
      "source": [
        "x_train, x_validation, y_train, y_validation = train_test_split(train_df, train_output_df, test_size=hp_test_size, random_state=42)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5OXy6FbTUR"
      },
      "source": [
        "# CREATE TENSORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8ZS9Lidh3R"
      },
      "source": [
        "def create_tensors(input_df):\n",
        "  stack = []\n",
        "  for column in input_df.columns:\n",
        "    if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
        "      stack.append(input_df[column].astype(np.float64))\n",
        "    else:\n",
        "      stack.append(input_df[column].cat.codes.values)\n",
        "  return torch.tensor(np.stack(stack, 1), dtype=torch.float)\n",
        "\n",
        "tensor_x_train_cat = create_tensors(x_train[categorical_columns]).float().to(device)\n",
        "tensor_x_train_num = create_tensors(x_train[numerical_columns]).float().to(device)\n",
        "tensor_y_train = torch.tensor(y_train.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_valid_cat = create_tensors(x_validation[categorical_columns]).float().to(device)\n",
        "tensor_x_valid_num = create_tensors(x_validation[numerical_columns]).float().to(device)\n",
        "tensor_y_valid = torch.tensor(y_validation.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_test_cat = create_tensors(test_df[categorical_columns]).float().to(device)\n",
        "tensor_x_test_num = create_tensors(test_df[numerical_columns]).float().to(device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRUVJlngYeT"
      },
      "source": [
        "# CREATE CATEGORICAL EMBEDDING SIZES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSDav1gcL2MN"
      },
      "source": [
        "categorical_columns_size = [len(df[column].cat.categories) for column in categorical_columns]\n",
        "categorical_embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_columns_size]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsI8RQbZhriV"
      },
      "source": [
        "# DEFINE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHX1GpfYkdHd"
      },
      "source": [
        "![](https://yashuseth.files.wordpress.com/2018/07/model1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1m9m_3Msst"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, embedding_size, input_size, num_numerical_cols, layers, ps):\n",
        "    super().__init__()\n",
        "\n",
        "    self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
        "    self.emb_drop = nn.Dropout(hp_emb_drop)\n",
        "\n",
        "    self.bn_cont = nn.BatchNorm1d(num_numerical_cols)\n",
        "\n",
        "    layerlist = []\n",
        "    for i, elem in enumerate(layers):\n",
        "      layerlist.append(nn.Linear(input_size, elem))\n",
        "      layerlist.append(nn.ReLU(inplace=True))\n",
        "      layerlist.append(nn.BatchNorm1d(layers[i]))\n",
        "      layerlist.append(nn.Dropout(ps[i]))\n",
        "      input_size = elem\n",
        "    layerlist.append(nn.Linear(layers[-1], 1))\n",
        "\n",
        "    self.layers = nn.Sequential(*layerlist)\n",
        "\n",
        "  def forward(self, x_c, x_n):\n",
        "\n",
        "    embeddings = [e(x_c[:,i].long()) for i, e in enumerate(self.all_embeddings)]\n",
        "\n",
        "    x = torch.cat(embeddings, 1)\n",
        "    x = self.emb_drop(x)\n",
        "\n",
        "    x_n = self.bn_cont(x_n)\n",
        "\n",
        "    x = torch.cat([x, x_n], 1)\n",
        "    x = self.layers(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHlbmRfnhjA"
      },
      "source": [
        "# INSTANTIATE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_wzONa0zWIv"
      },
      "source": [
        "try: del model, loss_function, optimizer\n",
        "except NameError: pass"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBAifb1TTjhG"
      },
      "source": [
        "num_numerical_cols = tensor_x_train_num.shape[1]\n",
        "\n",
        "num_categorical_cols = sum((nf for ni, nf in categorical_embedding_sizes))\n",
        "initial_input_size = num_categorical_cols + num_numerical_cols\n",
        "\n",
        "model = Model(categorical_embedding_sizes, initial_input_size, num_numerical_cols, layers=hp_layers, ps=hp_ps)\n",
        "sigmoid = nn.Sigmoid()\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hp_lr)\n",
        "model.to(device)\n",
        "tot_losses = []\n",
        "tot_auc = []"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTtyHh92n4Ac"
      },
      "source": [
        "# TRAIN NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yyOYgr5bZP"
      },
      "source": [
        "train_tensor_dataset = TensorDataset(tensor_x_train_cat, tensor_x_train_num, tensor_y_train)\n",
        "train_loader = DataLoader(dataset=train_tensor_dataset, batch_size=hr_batch_size, shuffle=True)\n",
        "\n",
        "start_training = generate_timestamp()\n",
        "\n",
        "model.train()\n",
        "\n",
        "tot_y_train_in = []\n",
        "tot_y_train_out = []\n",
        "\n",
        "for epoch in range(hp_epochs):\n",
        "  train_losses = []\n",
        "  for x_cat, x_num, y in train_loader:\n",
        "    y_train = model(x_cat, x_num)\n",
        "    single_loss = loss_function(sigmoid(y_train.squeeze()), y)\n",
        "    single_loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(single_loss.item())\n",
        "    tot_y_train_in.append(y)\n",
        "    tot_y_train_out.append(y_train)\n",
        "  epoch_loss = 1.0 * sum(train_losses) / len(train_losses)\n",
        "  tot_losses.append(epoch_loss)\n",
        "  epoch_auc = roc_auc_score(torch.cat(tot_y_train_in).cpu().numpy(), torch.cat(tot_y_train_out).cpu().detach().numpy())\n",
        "  tot_auc.append(epoch_auc)\n",
        "  tot_y_train_in = []\n",
        "  tot_y_train_out = []\n",
        "  print(\"epoch: \" + str(epoch) + \"\\tloss: \" + str(epoch_loss) + \"\\tauc: \" + str(epoch_auc))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "axes[0].plot(tot_losses)\n",
        "axes[1].plot(tot_auc)\n",
        "fig.tight_layout()\n",
        "\n",
        "last_train_loss = epoch_loss\n",
        "end_training = generate_timestamp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88QHnDQvGnQ"
      },
      "source": [
        "# VALIDATE NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozr6OavjNgrK"
      },
      "source": [
        "validation_tensor_dataset = TensorDataset(tensor_x_valid_cat, tensor_x_valid_num, tensor_y_valid)\n",
        "validation_loader = DataLoader(dataset=validation_tensor_dataset, batch_size=hr_batch_size, shuffle=True)\n",
        "\n",
        "valid_losses = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "tot_y_valid_in = []\n",
        "tot_y_valid_out = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for x_cat, x_num, y in validation_loader:\n",
        "    y_valid = model(x_cat, x_num)\n",
        "    validation_loss = loss_function(sigmoid(y_valid.squeeze()), y)\n",
        "    valid_losses.append(validation_loss.item())\n",
        "\n",
        "    tot_y_valid_in.append(y_valid)\n",
        "    tot_y_valid_out.append(y)\n",
        "\n",
        "  valid_loss = round(1.0 * sum(valid_losses) / len(valid_losses), 5)\n",
        "  print(\"loss: \" + str(valid_loss))\n",
        "  valid_auc = roc_auc_score(torch.cat(tot_y_valid_out).cpu(), torch.cat(tot_y_valid_in).cpu())\n",
        "  print(\"auc: \" + str(valid_auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9QC2cAvNrS"
      },
      "source": [
        "# MAKE PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbY5ybTLJpd4"
      },
      "source": [
        "with torch.no_grad():\n",
        "  y_test = model(tensor_x_test_cat, tensor_x_test_num)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7jy7-28XNpI"
      },
      "source": [
        "# VERIFIY PREDICTION DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mAYWZpsVJPW"
      },
      "source": [
        "nn_prediction_df = pd.DataFrame(y_test).astype(\"float\")\n",
        "x_scaled = min_max_scaler.fit_transform(nn_prediction_df)\n",
        "nn_prediction_df = pd.DataFrame(x_scaled)\n",
        "nn_prediction_df = pd.concat([nn_prediction_df, application_test_df['SK_ID_CURR']], axis=1)\n",
        "nn_prediction_df.columns = ['TEMP_TARGET', 'SK_ID_CURR']\n",
        "nn_prediction_df['TARGET'] = nn_prediction_df['TEMP_TARGET']\n",
        "nn_prediction_df = nn_prediction_df[['SK_ID_CURR', 'TARGET']]\n",
        "nn_prediction_df['TARGET'].hist(bins=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KntnKcEfIMuz"
      },
      "source": [
        "temp_prediction_df = nn_prediction_df.copy()\n",
        "temp_prediction_df['TARGET'] = round(temp_prediction_df['TARGET'], 1) \n",
        "test_target_mean = str(round(temp_prediction_df['TARGET'].mean(), 3))\n",
        "test_distrbution = temp_prediction_df.groupby(by=['TARGET'])['TARGET'].count()\n",
        "print(\"test_target_mean:\", test_target_mean)\n",
        "print(test_distrbution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5CHX5POvVYB"
      },
      "source": [
        "# SAVE PREDICTIONS TO CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPNd06TypiC0"
      },
      "source": [
        "nn_prediction_id = ''.join(random.choice(string.ascii_uppercase + string.digits) for i in range(8))\n",
        "nn_prediction_df.to_csv('/drive/My Drive/Notebooks/kaggle/HomeCreditDefaultRisk/submissions/' + nn_prediction_id + '.csv', index=False)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg2sCUB3aOdy"
      },
      "source": [
        "# SAVE DATA TO SHEET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA80EJskMH6E"
      },
      "source": [
        "model_values_dict = {\n",
        "  'ID': nn_prediction_id,\n",
        "  'start_training': start_training,\n",
        "  'end_training': end_training,\n",
        "  'perc_test_size': hp_test_size,\n",
        "  'emb_drop': hp_emb_drop,\n",
        "  'layers': '\\n'.join([str(i) for i in hp_layers]),\n",
        "  'ps': '\\n'.join([str(i) for i in hp_ps]),\n",
        "  'lr': hp_lr,\n",
        "  'epochs': hp_epochs,\n",
        "  'batch_size': hr_batch_size,\n",
        "  'train_losses': '\\n'.join([str(round(i, 5)) for i in tot_losses]),\n",
        "  'last_train_loss': last_train_loss,\n",
        "  'valid_loss': valid_loss,\n",
        "  'Δloss%': str(round((valid_loss / epoch_loss - 1) * 100, 3)) + '%',\n",
        "  'test_target_mean': test_target_mean,\n",
        "  'test_distrbution': test_distrbution.to_string(header=False),\n",
        "  'numerical_columns': len(numerical_columns),\n",
        "  'categorical_columns': len(categorical_columns),\n",
        "  'model_parameters': str(model.parameters)\n",
        "}\n",
        "\n",
        "next_row = get_next_row(sheet)\n",
        "cells = sheet.range('A' + next_row + ':S' + next_row)\n",
        "model_values_list = list(model_values_dict.values())\n",
        "for i, cell in enumerate(cells):\n",
        "  cell.value = model_values_list[i]\n",
        "sheet.update_cells(cells)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwecUUqkITLz"
      },
      "source": [
        "# XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc_ijeV5IUQR"
      },
      "source": [
        "\"\"\"\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "ratio = (train_output_df == 0).sum() / (train_output_df == 1).sum()[0]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ4mhPimKc3Y"
      },
      "source": [
        "\"\"\"\n",
        "lbl = preprocessing.LabelEncoder()\n",
        "train_df['CODE_GENDER'] = lbl.fit_transform(train_df['CODE_GENDER'].astype(str))\n",
        "train_df['FLAG_OWN_CAR'] = lbl.fit_transform(train_df['FLAG_OWN_CAR'].astype(str))\n",
        "train_df['NAME_EDUCATION_TYPE'] = lbl.fit_transform(train_df['NAME_EDUCATION_TYPE'].astype(str))\n",
        "train_df['FLAG_OWN_REALTY'] = lbl.fit_transform(train_df['FLAG_OWN_REALTY'].astype(str))\n",
        "train_df['OCCUPATION_TYPE'] = lbl.fit_transform(train_df['OCCUPATION_TYPE'].astype(str))\n",
        "train_df['ORGANIZATION_TYPE'] = lbl.fit_transform(train_df['ORGANIZATION_TYPE'].astype(str))\n",
        "train_df['has_only_approved'] = lbl.fit_transform(train_df['has_only_approved'].astype(str))\n",
        "train_df['has_been_rejected'] = lbl.fit_transform(train_df['has_been_rejected'].astype(str))\n",
        "train_df['has_job'] = lbl.fit_transform(train_df['has_job'].astype(str))\n",
        "train_df['cluster_days_employed'] = lbl.fit_transform(train_df['cluster_days_employed'].astype(str))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-z2-AD5IU7Y"
      },
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(train_df, train_output_df, test_size=0.2, stratify=train_output_df, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULEvd2PCI-ii"
      },
      "source": [
        "\"\"\"\n",
        "clf = XGBClassifier(n_estimators=1000, objective='binary:logistic', gamma=0.1, subsample=0.5, scale_pos_weight=ratio[0])\n",
        "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='auc', early_stopping_rounds=10)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-7DajbyPcA9"
      },
      "source": [
        "\"\"\"\n",
        "test_df['CODE_GENDER'] = lbl.fit_transform(test_df['CODE_GENDER'].astype(str))\n",
        "test_df['FLAG_OWN_CAR'] = lbl.fit_transform(test_df['FLAG_OWN_CAR'].astype(str))\n",
        "test_df['NAME_EDUCATION_TYPE'] = lbl.fit_transform(test_df['NAME_EDUCATION_TYPE'].astype(str))\n",
        "test_df['FLAG_OWN_REALTY'] = lbl.fit_transform(test_df['FLAG_OWN_REALTY'].astype(str))\n",
        "test_df['OCCUPATION_TYPE'] = lbl.fit_transform(test_df['OCCUPATION_TYPE'].astype(str))\n",
        "test_df['ORGANIZATION_TYPE'] = lbl.fit_transform(test_df['ORGANIZATION_TYPE'].astype(str))\n",
        "test_df['has_only_approved'] = lbl.fit_transform(test_df['has_only_approved'].astype(str))\n",
        "test_df['has_been_rejected'] = lbl.fit_transform(test_df['has_been_rejected'].astype(str))\n",
        "test_df['has_job'] = lbl.fit_transform(test_df['has_job'].astype(str))\n",
        "test_df['cluster_days_employed'] = lbl.fit_transform(test_df['cluster_days_employed'].astype(str))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi1ocuBvPTgU"
      },
      "source": [
        "\"\"\"\n",
        "xgboost_prediction_df = clf.predict_proba(test_df)[:, 1]\n",
        "xgboost_prediction_df = pd.DataFrame({'SK_ID_CURR': application_test_df['SK_ID_CURR'].values, 'TARGET': xgboost_prediction_df})\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA3KYzY4Go6I"
      },
      "source": [
        "# ENSEMBLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zeuj0BSGqoH"
      },
      "source": [
        "\"\"\"\n",
        "ensemble_df = pd.merge(submission_df, nn_prediction_id, on='SK_ID_CURR', how='left')\n",
        "ensemble_df['TARGET'] = (ensemble_df['TARGET_x'] + ensemble_df['TARGET_y']) / 2\n",
        "ensemble_df = ensemble_df[['SK_ID_CURR', 'TARGET']]\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTnRg6t1P8Tl"
      },
      "source": [
        "# DOWNLOAD RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxXyeR21P7jK"
      },
      "source": [
        "# namestr(df, globals())[0]\n",
        "# nn_prediction_df\n",
        "# xgboost_prediction_df\n",
        "# ensemble_df\n",
        "# ensemble_df.to_csv('submission.csv', index=False)\n",
        "# files.download('submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}