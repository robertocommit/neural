{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Home Credit Default Risk",
      "provenance": [],
      "collapsed_sections": [
        "s933KG08D48G"
      ],
      "authorship_tag": "ABX9TyMUOjsEGnDPC1EJKCqfpUA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robimalco/colab/blob/main/Home_Credit_Default_Risk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9Cm7RzTvoY"
      },
      "source": [
        "# ADD KEYS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPDViyQqPsaM"
      },
      "source": [
        "!rm -rf *\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"robimalco\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"0f4600b98146814d7a153f58c4cf4389\" # key from the json file\n",
        "!pip install -q kaggle\n",
        "!kaggle competitions download -c home-credit-default-risk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdr6gDQq5rET"
      },
      "source": [
        "# START SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KnrADYj7yUe"
      },
      "source": [
        "!unzip application_test.csv.zip\n",
        "!unzip application_train.csv.zip\n",
        "# !unzip POS_CASH_balance.csv.zip\n",
        "# !unzip bureau.csv.zip\n",
        "# !unzip bureau_balance.csv.zip\n",
        "# !unzip credit_card_balance.csv.zip\n",
        "# !unzip installments_payments.csv.zip\n",
        "# !unzip previous_application.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbHvWyux-QjF"
      },
      "source": [
        "!pip install torch==1.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import random\n",
        "import string\n",
        "letters = string.ascii_lowercase"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWBMpGHRRgn-"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGDv_sUBXxI"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnY9FfmTaHux"
      },
      "source": [
        "!pip install --upgrade gspread\n",
        "from google.colab import auth, drive\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "drive.mount('/drive')\n",
        "auth.authenticate_user()\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1M-CqsTbBu7ScY4mZBcpI8kPbI7F-BE8aPTC4UknumYk/edit#gid=0')\n",
        "sheet = wb.worksheet('Data')\n",
        "def get_next_row(worksheet):\n",
        "    str_list = list(filter(None, worksheet.col_values(1)))\n",
        "    return str(len(str_list)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ4CWCK1cvvb"
      },
      "source": [
        "# SET HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xgR3YWAcuZk"
      },
      "source": [
        "hp_test_size = 0.2\n",
        "hp_emb_drop = 0.04\n",
        "hp_layers = [1000, 500]\n",
        "hp_ps = [0.001,0.01]\n",
        "hp_lr= 0.001\n",
        "hp_epochs = 3"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxeWyJnq58bz"
      },
      "source": [
        "# Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F40xkOit6Df6"
      },
      "source": [
        "*   application_{train|test}.csv --> main table, static data for all applications. One row represents one loan in our data sample.\n",
        "*   bureau.csv -->  client's previous credits, for every loan in our sample, there are as many rows as number of credits the client had.\n",
        "*   bureau_balance.csv --> monthly balances of previous credits, one row for each month.\n",
        "*   POS_CASH_balance.csv --> monthly balance snapshots of previous point of sales and cash loans that the applicant had, one row for each month.\n",
        "*   credit_card_balance.csv --> monthly balance snapshots of previous credit cards, one row for each month.\n",
        "*   previous_application.csv --> all previous applications for Home Credit loans of clients who have loans.\n",
        "*   installments_payments.csv --> repayment history for the previously disbursed credits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iS2krKz9hVR"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoAxsP_l9glq"
      },
      "source": [
        "application_train_df = pd.read_csv('application_train.csv').sample(frac = 1)\n",
        "application_test_df = pd.read_csv('application_test.csv')\n",
        "# bureau_df = pd.read_csv('bureau.csv')\n",
        "# bureau_balance_df = pd.read_csv('bureau_balance.csv')\n",
        "# pos_cash_balance_df = pd.read_csv('POS_CASH_balance.csv')\n",
        "# credit_card_balance_df = pd.read_csv('credit_card_balance.csv')\n",
        "# previous_application_df = pd.read_csv('previous_application.csv')\n",
        "# installments_payments_df = pd.read_csv('installments_payments.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxIIM5xLxla9"
      },
      "source": [
        "# application_train_df = application_train_df.head(round(application_train_df.shape[0] * 0.5))\n",
        "# application_test_df = application_test_df.head(round(application_test_df.shape[0] * 0.3))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp3mbkvTIboq"
      },
      "source": [
        "application_train_df['CSV_SOURCE'] = 'application_train.csv'\n",
        "application_test_df['CSV_SOURCE'] = 'application_test.csv'\n",
        "df = pd.concat([application_train_df, application_test_df])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s933KG08D48G"
      },
      "source": [
        "# UTILITIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1r_TCDSbOWB"
      },
      "source": [
        "def generate_timestamp():\n",
        "  now = datetime.datetime.now()\n",
        "  year = '{:02d}'.format(now.year)\n",
        "  month = '{:02d}'.format(now.month)\n",
        "  day = '{:02d}'.format(now.day)\n",
        "  hour = '{:02d}'.format(now.hour)\n",
        "  minute = '{:02d}'.format(now.minute)\n",
        "  return '{}-{}-{} {}:{}'.format(year, month, day, hour, minute)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlhVH--sUJ9u"
      },
      "source": [
        "def order_columns_alphabetically(input_df):\n",
        "  input_df_columns = list(input_df.columns)\n",
        "  input_df_columns.sort()\n",
        "  return input_df[input_df_columns]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMpsQ-_X-996"
      },
      "source": [
        "def split_columns_by_type(input_df):\n",
        "  numerical_columns = []\n",
        "  categorical_columns = []\n",
        "  for column in input_df.columns:\n",
        "    if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
        "      numerical_columns.append(column)\n",
        "    else:\n",
        "      categorical_columns.append(column)\n",
        "  return numerical_columns, categorical_columns"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ThF3xmfqKU"
      },
      "source": [
        "# threshold = 60\n",
        "def convert_numerical_to_categorical(input_df):\n",
        "  for column in input_df.columns:\n",
        "    if column == 'TARGET':\n",
        "      pass\n",
        "    elif input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
        "      if len(input_df[column].unique()) < 60:\n",
        "        input_df[column] = input_df[column].astype('string')\n",
        "  return input_df"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYbnyF3dTGWo"
      },
      "source": [
        "def smart_overview(input_df):\n",
        "  a_types = []\n",
        "  a_countUnique = []\n",
        "  a_missing = []\n",
        "  a_missing_perc = []\n",
        "  a_corrTarget = []\n",
        "  a_min = []\n",
        "  a_max = []\n",
        "  a_mean = []\n",
        "  a_quantile = []\n",
        "  for column in input_df.columns:\n",
        "    x = input_df[column]\n",
        "    x_type = input_df.dtypes[column]\n",
        "    countUnique = len(x.unique())\n",
        "    missing = x.isnull().sum()\n",
        "    missing_perc = round((missing/input_df.shape[0]),3)*100\n",
        "    if x_type == np.int64 or x_type == np.float64:\n",
        "      a_corrTarget.append(round(x.corr(input_df['TARGET']), 3))\n",
        "      a_min.append(x.min())\n",
        "      a_max.append(x.max())\n",
        "      a_mean.append(x.mean())\n",
        "      a_quantile.append(x.quantile(0.5))\n",
        "    else:\n",
        "      a_corrTarget.append('')\n",
        "      a_min.append('')\n",
        "      a_max.append('')\n",
        "      a_mean.append('')\n",
        "      a_quantile.append('')\n",
        "    a_types.append(x_type)\n",
        "    a_countUnique.append(countUnique)\n",
        "    a_missing.append(missing)\n",
        "    a_missing_perc.append(missing_perc)\n",
        "  explore_df = pd.DataFrame({\n",
        "    'Columns': input_df.columns,\n",
        "    'Types': a_types,\n",
        "    'Unique': a_countUnique,\n",
        "    'Missing': a_missing,\n",
        "    'Missing%': a_missing_perc,\n",
        "    'CorrTarget': a_corrTarget,\n",
        "    'Min': a_min,\n",
        "    'Max': a_max,\n",
        "    'Mean': a_mean,\n",
        "    'Quantile': a_quantile\n",
        "  })\n",
        "  explore_df.set_index('Columns', inplace=True)\n",
        "  return explore_df.transpose()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmxzAril9NMB"
      },
      "source": [
        "# MANAGE COLUMNS (NUMERICAL VS CATEGORICAL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTJtbRPtDRn9"
      },
      "source": [
        "df = order_columns_alphabetically(df)\n",
        "df = convert_numerical_to_categorical(df)\n",
        "columns_type = split_columns_by_type(df)\n",
        "numerical_columns = columns_type[0]\n",
        "categorical_columns = columns_type[1]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEy-9Qsz7ZoM"
      },
      "source": [
        "numerical_columns = ['AMT_ANNUITY', 'AMT_CREDIT']\n",
        "categorical_columns = ['CODE_GENDER', 'CSV_SOURCE', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']\n",
        "target_column = ['TARGET']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPTBqQsV8VUB"
      },
      "source": [
        "df = df[numerical_columns + categorical_columns + target_column]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDybyT2XAc5P"
      },
      "source": [
        "smart_overview(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l96wo_5oYYA3"
      },
      "source": [
        "# MANAGE MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2j9zI7mHBoP"
      },
      "source": [
        "for numerical_column in numerical_columns:\n",
        "  df[numerical_column].fillna(value=df[numerical_column].median(), inplace=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeMm7CCwYgi0"
      },
      "source": [
        "# STANDARDISE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT7t1r068wJX"
      },
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df[numerical_columns] = pd.DataFrame(min_max_scaler.fit_transform(df[numerical_columns]))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y66MJfs5ZBUC"
      },
      "source": [
        "# CONVERT CATEGORICAL COLUMNS INTO TYPE \"CATEGORY\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Hv3Ii6DWMx"
      },
      "source": [
        "categorical_columns.remove('CSV_SOURCE')\n",
        "\n",
        "for column in categorical_columns:\n",
        "  df[column] = LabelEncoder().fit_transform(df[column].astype(str))\n",
        "  df[column] = df[column].astype('category')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lmf7AyiaqGL"
      },
      "source": [
        "# SPLIT DATA INTO TRAINING vs TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB6lrtUpJan-"
      },
      "source": [
        "train_df = df[df['CSV_SOURCE'] == 'application_train.csv']\n",
        "train_output_df = pd.DataFrame(train_df['TARGET'], columns=['TARGET'])\n",
        "\n",
        "test_df = df[df['CSV_SOURCE'] == 'application_test.csv']"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7yzRWz9a-UZ"
      },
      "source": [
        "# REMOVE NOT USEFUL COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKJ3_8sMbC_U"
      },
      "source": [
        "train_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)\n",
        "test_df.drop(columns=['CSV_SOURCE', 'TARGET'], axis=0, inplace=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XyQsFRcpSZ"
      },
      "source": [
        "# CREATE VALIDATION SET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Isjke6nLcV2G"
      },
      "source": [
        "x_train, x_validation, y_train, y_validation = train_test_split(train_df, train_output_df, test_size=hp_test_size, random_state=42)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5OXy6FbTUR"
      },
      "source": [
        "# CREATE TENSORS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8ZS9Lidh3R"
      },
      "source": [
        "def create_tensors(input_df):\n",
        "  stack = []\n",
        "  for column in input_df.columns:\n",
        "    if input_df.dtypes[column] == np.int64 or input_df.dtypes[column] == np.float64:\n",
        "      stack.append(input_df[column].astype(np.float64))\n",
        "    else:\n",
        "      stack.append(input_df[column].cat.codes.values)\n",
        "  return torch.tensor(np.stack(stack, 1), dtype=torch.float)\n",
        "\n",
        "tensor_x_train_cat = create_tensors(x_train[categorical_columns]).float().to(device)\n",
        "tensor_x_train_num = create_tensors(x_train[numerical_columns]).float().to(device)\n",
        "tensor_y_train = torch.tensor(y_train.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_valid_cat = create_tensors(x_validation[categorical_columns]).float().to(device)\n",
        "tensor_x_valid_num = create_tensors(x_validation[numerical_columns]).float().to(device)\n",
        "tensor_y_valid = torch.tensor(y_validation.values).flatten().float().to(device)\n",
        "\n",
        "tensor_x_test_cat = create_tensors(test_df[categorical_columns]).float().to(device)\n",
        "tensor_x_test_num = create_tensors(test_df[numerical_columns]).float().to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRUVJlngYeT"
      },
      "source": [
        "# CREATE CATEGORICAL EMBEDDING SIZES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSDav1gcL2MN"
      },
      "source": [
        "categorical_columns_size = [len(df[column].cat.categories) for column in categorical_columns]\n",
        "categorical_embedding_sizes = [(col_size, min(50, (col_size + 1) // 2)) for col_size in categorical_columns_size]"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJEKKoLhQAz"
      },
      "source": [
        "# CREATE PYTORCH DATA LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaVwMCL64ttq"
      },
      "source": [
        "train_tensor_dataset = TensorDataset(tensor_x_train_cat, tensor_x_train_num, tensor_y_train)\n",
        "train_loader = DataLoader(dataset=train_tensor_dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsI8RQbZhriV"
      },
      "source": [
        "# DEFINE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHX1GpfYkdHd"
      },
      "source": [
        "![](https://yashuseth.files.wordpress.com/2018/07/model1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_1m9m_3Msst"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, embedding_size, input_size, num_numerical_cols, layers, ps):\n",
        "    super().__init__()\n",
        "\n",
        "    self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
        "    self.emb_drop = nn.Dropout(hp_emb_drop)\n",
        "\n",
        "    self.bn_cont = nn.BatchNorm1d(num_numerical_cols)\n",
        "\n",
        "    layerlist = []\n",
        "    for i, elem in enumerate(layers):\n",
        "      layerlist.append(nn.Linear(input_size, elem))\n",
        "      layerlist.append(nn.ReLU(inplace=True))\n",
        "      layerlist.append(nn.BatchNorm1d(layers[i]))\n",
        "      layerlist.append(nn.Dropout(ps[i]))\n",
        "      input_size = elem\n",
        "    layerlist.append(nn.Linear(layers[-1], 1))\n",
        "\n",
        "    self.layers = nn.Sequential(*layerlist)\n",
        "\n",
        "  def forward(self, x_c, x_n):\n",
        "\n",
        "    embeddings = [e(x_c[:,i].long()) for i, e in enumerate(self.all_embeddings)]\n",
        "\n",
        "    x = torch.cat(embeddings, 1)\n",
        "    x = self.emb_drop(x)\n",
        "\n",
        "    x_n = self.bn_cont(x_n)\n",
        "\n",
        "    x = torch.cat([x, x_n], 1)\n",
        "    x = self.layers(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHlbmRfnhjA"
      },
      "source": [
        "# INSTANTIATE NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBAifb1TTjhG"
      },
      "source": [
        "num_numerical_cols = tensor_x_train_num.shape[1]\n",
        "\n",
        "num_categorical_cols = sum((nf for ni, nf in categorical_embedding_sizes))\n",
        "initial_input_size = num_categorical_cols + num_numerical_cols\n",
        "\n",
        "model = Model(categorical_embedding_sizes, initial_input_size, num_numerical_cols, layers=hp_layers, ps=hp_ps)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=hp_lr)\n",
        "model.to(device)\n",
        "tot_losses = []"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTtyHh92n4Ac"
      },
      "source": [
        "# TRAIN NEURAL NETWORK MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6yyOYgr5bZP"
      },
      "source": [
        "start_training = generate_timestamp()\n",
        "\n",
        "for epoch in range(hp_epochs):\n",
        "  train_losses = []\n",
        "  for x_cat, x_num, y in train_loader:\n",
        "    y_pred = model(x_cat, x_num)\n",
        "    single_loss = torch.sqrt(loss_function(y_pred.squeeze(), y))\n",
        "    optimizer.zero_grad()\n",
        "    single_loss.backward() \n",
        "    optimizer.step()\n",
        "    train_losses.append(single_loss.item())\n",
        "  epoch_loss = 1.0 * sum(train_losses) / len(train_losses)\n",
        "  tot_losses.append(epoch_loss)\n",
        "  print(\"epoch: \" + str(epoch) + \"\\tloss: \" + str(epoch_loss))\n",
        "plt.plot(tot_losses)\n",
        "plt.show()\n",
        "\n",
        "end_training = generate_timestamp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d88QHnDQvGnQ"
      },
      "source": [
        "# VALIDATE NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozr6OavjNgrK"
      },
      "source": [
        "validation_tensor_dataset = TensorDataset(tensor_x_valid_cat, tensor_x_valid_num, tensor_y_valid)\n",
        "validation_loader = DataLoader(dataset=validation_tensor_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "valid_losses = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for x_cat, x_num, y in validation_loader:\n",
        "    y_valid = model(x_cat, x_num)\n",
        "    validation_loss = torch.sqrt(loss_function(y_valid.squeeze(), y))\n",
        "    valid_losses.append(validation_loss.item())\n",
        "  valid_loss = str(round(1.0 * sum(valid_losses) / len(valid_losses), 5))\n",
        "  print(\"loss: \" + valid_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q9QC2cAvNrS"
      },
      "source": [
        "# MAKE PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbY5ybTLJpd4"
      },
      "source": [
        "with torch.no_grad():\n",
        "  y_test = model(tensor_x_test_cat, tensor_x_test_num)\n",
        "pd.DataFrame(y_test).astype(\"float\").hist(bins=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5CHX5POvVYB"
      },
      "source": [
        "# SAVE PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc2BwhwKoHDa"
      },
      "source": [
        "prediction_id = ''.join(random.choice(letters) for i in range(5))\n",
        "prediction_df = pd.DataFrame(y_test).astype(\"float\")\n",
        "x_scaled = min_max_scaler.fit_transform(prediction_df)\n",
        "prediction_df = pd.DataFrame(x_scaled)\n",
        "prediction_df = pd.concat([prediction_df, application_test_df['SK_ID_CURR']], axis=1)\n",
        "prediction_df.columns = ['TEMP_TARGET', 'SK_ID_CURR']\n",
        "prediction_df['TARGET'] = round(prediction_df['TEMP_TARGET'], 1)\n",
        "prediction_df = prediction_df[['SK_ID_CURR', 'TARGET']]\n",
        "prediction_df.to_csv('/drive/My Drive/pycharm_colab_training/kaggle/HomeCreditDefaultRisk/submissions/' + prediction_id + '.csv', index=False)\n",
        "test_target_mean = str(round(prediction_df['TARGET'].mean(), 3))\n",
        "test_distrbution = prediction_df.groupby(by=['TARGET'])['TARGET'].count()\n",
        "print(\"test_target_mean:\", test_target_mean)\n",
        "print(test_distrbution.to_string(header=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg2sCUB3aOdy"
      },
      "source": [
        "# SAVE DATA TO SHEET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA80EJskMH6E"
      },
      "source": [
        "model_values_dict = {\n",
        "  'ID': prediction_id,\n",
        "  'start_training': start_training,\n",
        "  'end_training': end_training,\n",
        "  'perc_test_size': hp_test_size,\n",
        "  'emb_drop': hp_emb_drop,\n",
        "  'layers': '\\n'.join([str(i) for i in hp_layers]),\n",
        "  'ps': '\\n'.join([str(i) for i in hp_ps]),\n",
        "  'lr': hp_lr,\n",
        "  'epochs': hp_epochs,\n",
        "  'train_losses': '\\n'.join([str(round(i, 5)) for i in tot_losses]),\n",
        "  'valid_loss': valid_loss,\n",
        "  'test_target_mean': test_target_mean,\n",
        "  'test_distrbution': test_distrbution.to_string(header=False),\n",
        "  'numerical_columns': len(numerical_columns),\n",
        "  'categorical_columns': len(categorical_columns),\n",
        "  'model_parameters': str(model.parameters)\n",
        "}\n",
        "\n",
        "next_row = get_next_row(sheet)\n",
        "cells = sheet.range('A' + next_row + ':P' + next_row)\n",
        "model_values_list = list(model_values_dict.values())\n",
        "for i, cell in enumerate(cells):\n",
        "  cell.value = model_values_list[i]\n",
        "sheet.update_cells(cells)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}